% !TeX root = thesis.tex

%	Oskar L F LeirvÃ¥g
%	Masters Thesis

\documentclass[a4paper]{article}
\input{extras/packages.tex}
\input{extras/commands.tex}

\begin{document}

% % % % % % % % % % % % % % % % %
%
%	FRONT PAGE
%
\input{extras/frontpage.tex}

% % % % % % % % % % % % % % % % %
%
%	Acknowledgements
%
\section*{Acknowledgements}
empty
\newpage

% % % % % % % % % % % % % % % % %
%
%	0. Abstract
%
\section*{Abstract}
empty
\newpage

% % % % % % % % % % % % % % % % %
%
%	TABLE OF CONTENTS
%
\pdfbookmark{\contentsname}{toc}
\tableofcontents
\newpage

% % % % % % % % % % % % % % % % %
%
%	TABLE OF ALGORITHMS
%
\addcontentsline{toc}{section}{List of Algorithms}
\listofalgorithms
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % %
%
%	1. Introduction
%
\section{Introduction}
In the current age of technology, humans have gathered unfathomable amounts of data wherever
we saw the possibility to log something. Ranging from major corporations to single individuals,
as long as we do anything digital, data is being logged, and the amount is measured in Exabytes.
It's well known that there is useful information not only in the direct statistics of this data,
but in the anomalies, patterns and correlations between the data. The process of looking though and
comparing all this data, has become known as Data mining, due to the complexity of the work,
and time required.

One technique of many to find correlations between data in big datasets is clustering. While
clustering, we in general look on data-points and organize the data-points
into sets; where any data-point within a set, is more similar to all other data-points inside
the same set, than all other data-points in other sets. Hence by adding any new
data-point, we should be able to find which cluster or classification-set it is closest too,
thereby also classifying it. \cite{gan07}

Now suppose we have a streaming webpage that collects data on whether a user like any given movie or not,
then we might want to use this data to recommend movies to any specific user which they might like.
If we can classify our users into groups that share mostly the same taste in movies, then
the "average-user" or centres of the classifications would be a "super user" most likely to share the same movie
interest as any other user in the same classification.

To quickly group the new users, we can use these averages or centres. This is called centroid-based
clustering, so that all data-points can be linearly classified by finding it's closest centre.
The problem then becomes defining what is a centre, and how do we find them.

Unfortunately we now know that to cluster and create these centres is an NP-hard problem, 
and to create such centres for small instances might not be viable even with massive computing power,
over long time frames.

\subsection{Motivation}
Much research has gone into developing theoretical algorithms for the most complex computational
problems in computer science, however many of them remain purely theoretical, not because
they're not useful, but because the implementation of brute-force algorithms is simpler, and
computational power can be bought. Testing the viability and practicality of implementing
these complex algorithms is an important step in the research done for improving the
algorithms, and the many practical limitations we face while constructing them in our
preferred languages.

This paper aims to clarify the issues faced during development, and the results one could expect
on a practical level of computation time on a smaller home computer/laptop.

\subsection{Problem statement}
From example above with our streaming webpage, we get a rather unique situation where the problem consist 
purely of binary data-points. We want to find a viable centroid-based clustering algorithm, to find
centres in clusters of binary vectors, in a reasonable runtime.

For this paper, we will look into a theoretical centroid-based binary vector clustering algorithm, proposed
by \cite{fomin_golovach_panolan_2020}. First we need to find a distance metric between the vectors so we 
can group them. The algorithm from \cite{fomin_golovach_panolan_2020} has define the clustering metric 
to be the Hamming distance (\textit{$d_H(a, b)$}) between the binary vectors, such that the centres 
becomes the averages of all bits in a row inside the cluster. Due to the algorithm using both rows and 
columns, it uses a matrix to represent the set of vectors as the columns.

This propose theoretical algorithm is a parameterized algorithm. Here we are limiting the runtime of the
algorithm parameterized by the two parameters $k$ and $r$, where $k$ is the total maximum deviancy 
of all data-points to their respective centres, and $r$ is the maximum allowed sets and 
thereby total centres allowed in a solution.

\newpage

Now let's formally define the problem then, of "Binary r-Means".
\begin{problem}[Binary r-Means]{prob:rm}
\begin{tabular}{p{0.1\textwidth}p{0.8\textwidth}}
    \textit{Input}: & An $n \times m$ binary matrix \textbf{A} with columns
    ($\textbf{a}^1,...,\textbf{a}^n$), a positive integer $\textbf{r}$ and a nonnegative
    integer $\textbf{k}$                                                                     \\

    \textit{Task}:  & Decide whether there is a positive integer $\textbf{r}\sp{\prime} \leq
        \textbf{r}$, a partition $\{I_1, ..., I_{r\sp{\prime}}\}$ of $\{1,...,n\}$ and vectors
    $(\textbf{c}^1,...,\textbf{c}^{r\sp{\prime}}) \in \{0,1\}^m$ such that

    \[
        \sum_{i = 1}^{r\sp{\prime}} \sum_{j \in I_i} d_H(c^i, a^j) \leq k
    \]
\end{tabular}
\end{problem}

Now given any set $S$, it is trivial to find the centre of that set in polynomial time. The challenge
here is that we don't know the sets, and we need a centre to create the sets. It simply becomes a problem
of which comes first, the chicken or the egg.

Because we need to know the sets, before we can find the centres in the algorithm, we will
generally need to compare all data-points to all other data-points, creating a $\mathcal{O}(n!)$ runtime. This
is highly undesirable as only a few hundred vectors could result in a runtime longer than the lifetime
of the universe. The algorithm from the paper \cite{fomin_golovach_panolan_2020} has however proposed a 
runtime of $2^{\mathcal{O} (\sqrt{rk log(k+r) logr})}*nm$; placing the algorithm in the FPT class, which is
much closer to the runtime we might expect from normal computers.

\subsection{Example}
Now lets look back to our example. We want to find the "centres" or "super users" which we can use to 
give new movie recommendations to new users. Let's see how we have to encode this problem over a language
equal to the problem we stated above.

To convert this problem to a binary matrix, we can define each individual user as a vector
$u=\{0,1\}^m$ where the individual bit-number in the vector represent the same movie for all users
and indicates whether the user liked ('1') or disliked the movie ('0'), such that if the two
user-vectors $u=\{1,1,\ldots\}^m$ and $v=\{0,1,\ldots\}^m$ we say that $u$ and $v$ both like movie $2$,
but disagree on movie $1$ as $u_1 \neq v_1$.

We then transform the vectors and place them in the set $U^n$. Now we can use the user-vectors of $U$
as columns in a matrix such that
\[
    A(U)  \rightarrow \begin{pmatrix}
        1       & 0       & \cdots & a_{1,n} \\
        1       & 1       & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
\]

When running the algorithm we also have to select the parameters $k$ and $r$ where $r$ would be
how many centres of movie-personalities we wish to define, and $k$ how much the users in
the training-set would be allowed to deviate from the centre in total, also known as the cost.

The resulting vectors would now be the new set of super-users to use in a recommendation system,
linearly comparing every new user to the one centre they are closest to whenever they rate movies.

\subsection{Previous results}
empty

% % % % % % % % % % % % % % % % %
%
%	2. Preliminaries
%
\newpage

\section{Preliminaries}
\subsection{Notations and Definitions}
\subsubsection{Matrices}
As we are clustering binary vectors $v \in \{0,1\}^m$, we can transform any set of vectors
$V$ into a matrix $A(v^1, v^2, \cdots, v^n)$ where $n$ is the number of columns
and $m$ is the length of the vectors.

\[
    A_{m,n} =
    \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
\]

\subsubsection{Hamming distance}
For clustering we need to compare the columns of matrices by distance, to calculate the distance
between columns we will use the hamming distance. Let us recall that the hamming distance
between two binary vectors $x,y \in \{0,1\}^m$ where $x=\{x_1,...,x_m\}^T$ and
$y=\{y_1,...,y_m\}^T$, is

\[
    d_H(x,y)= \sum_{i = 1}^{m} |x_i - y_i|
\]

or, in other words, the number of positions $i \in \{1,...,m\}$ where $x_i$ and $y_i$ differ.

With that distance we can
also define the distance between two matrices as the sum of the distance between the columns of
the matrices. Now we can also define the cardinality of a binary vector as the number of positive bits in
the vector $v \in \{0,1\}^m$ such that $d_C(v)=\sum_{i = 1}^{m} v^i$. As the binary XOR ($\otimes$)
operation between two bits results $i \otimes j = |i - j|$, we see that hamming distance between
the two binary vectors can be defined

\[
    d_H(x, y) = d_C(x \otimes y) =\sum_{i = 1}^{m} (x^i \otimes y^i) = \sum_{i = 1}^{m} |x_i - y_i|
\]

For two binary $n \times m$ matrices with columns $A=(a^1,...,a^n)$ and
$B=(b^1,...,b^n)$, the hamming distance we define as

\[
    d_H(A,B) = \sum_{i = 1}^{n} d_H(a^i, b^i)
\]

\subsection{Parameterized algorithms}
As the main algorithm in this thesis is based on a parameterized problem, we here refer to the
book \cite{param_algo_book} for a detailed introduction. Here we shall only refer to the most
basic theorems.

\begin{theoremdefinition}[Parameterized problem]
    A \textit{parameterized problem} is a language $L \subseteq \Sigma^* \times \mathbb{N}$,
    where $\Sigma$ is a fixed, finite alphabet. For an instance $(x,k) \in \Sigma^* \times \mathbb{N}$,
    $k$ is called the \textit{parameter}. \cite[p.~12]{param_algo_book}
\end{theoremdefinition}

The problem of r-means is stated to be such a parameterized problem as any instance $(A,r,k)$
can be written as $(A, k)$ where $A$ is a binary matrix encoded as a string over $\Sigma$, and
$k$ is a positive integer.

Now it is also proposed that the algorithm proposed by \cite{fomin_golovach_panolan_2020} 
gives an FPT running time.

\begin{theoremdefinition}[Fixed-parameter tractable]
    A parameterized problem $L \subseteq \Sigma^* \times \mathbb{N}$ is called
    \textit{fixed-parameter tractable} (FPT) if there exists an algorithm $\mathcal{A}$ (called
    a \textit{fixed-parameter algorithm}), a computable function $f:\mathbb{N} \rightarrow \mathbb{N}$,
    and a constant $c$ such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$, the algorithm
    $\mathcal{A}$ correctly decides whether $(x,k) \in L$ in time bounded by $f(k) * |(x,k)|^c$.
    The complexity class containing all fixed-parameter tractable problems is called FPT \cite[p.~13]{param_algo_book}
\end{theoremdefinition}

By this definition the proposed algorithm from \cite{fomin_golovach_panolan_2020} should have 
a runtime in the form of $f(k) * |(x,k)|^c$. We can see that the proposed runtime is 

\begin{theoremdefinition}[Data reduction rule]
    A \textit{data reduction rule}, of a parameterized problem $Q$ is a function 
    $\phi \Sigma^* \times \mathbb{N}$ that maps an instance $(I,k)$ of $Q$ to an equivalent
    instance $(I^\prime, k^\prime)$ of $Q$ such that $\phi$ is computable in polynomial time
    in $|I|$ and $k$. We say that two instances of $Q$ are \textit{equivalent} if the following
    holds: $(I,k) \in Q$ if and only if $(I^\prime, k^\prime) \in Q$. 
    \cite{fomin_golovach_panolan_2020}
\end{theoremdefinition}

\begin{theoremdefinition}[Kernelization, kernel]
    A kernelization algorithm, or simply a kernel, for a parameterized problem $Q$
    is an algorithm $\mathcal{A}$ that, given an instance $(I,k)$ of $Q$, works in polynomial
    time and returns an equivalent instance $(I^\prime, k^\prime)$ of $Q$. Moreover, we require
    that $size_{\mathcal{A}}(k) \leq g(k)$ for some computable function
    $g: \mathbb{N} \rightarrow \mathbb{N}$. \cite[p.~18]{param_algo_book}
\end{theoremdefinition}

\begin{theoremdefinition}[Turing kernelization]
    Let $Q$ be a parameterized problem and let $f:\mathbb{N} \rightarrow \mathbb{N} $
    be a computable function. A \textit{Turing kernelization} for $Q$ of size $f$ is an
    algorithm that decides whether a given instance $(x,k) \in \Sigma^* \times \mathbb{N}$
    is contained in $Q$ in time polynomial in $|x|+k$, when given access to an oracle
    that decides membership in $Q$ for any instance $(x^\prime, k^\prime)$ with
    $|x^\prime|,k^\prime \leq f(k)$ in a single step. \cite[p.~314]{param_algo_book}
\end{theoremdefinition}

\subsection{Problem complexity / kernelization}

% % % % % % % % % % % % % % % % %
%
%	3. Algorithm
%
\newpage

\section{Algorithm}
\subsection{Kernelization}
The goal of the preprocessing algorithm is to reduce the input matrix by following certain reduction rules
based on the two parameters $k$ and $r$, running in polynomial time over the input size, here ($n \times m$).
We will here apply the following reduction-rules 
\begin{reductionrule}\label{red:equalclusters}
    Any duplicate vector must be in the same cluster. Therefore it follows that more than $k+2$ equal vector
    cannot impact the result and they can therefore be discarded.
\end{reductionrule}

\begin{reductionrule}\label{red:distmaxk}
    All vectors of distance $d_H \leq k$ should be in the same cluster. Therefore we should move all vectors of
    distance $d_H \leq k$ into the same.
\end{reductionrule}

\begin{reductionrule}\label{red:uniformrow}
    For all defined clusters, it must follow that any \textit{uniform row} will result in the same result. Meaning 
    if $\forall k$ in $\Sigma_{i=1}^{m} v_i \in A_k = 0 \vee m$, the row can be discarded.
\end{reductionrule}

When reducing the matrix the algorithm should produce a set of clusters $L$, where any vector in a cluster is at max $k$ distance to all
other vectors in the same cluster $\forall d_H(v, u) \in L_i \leq k$. Each of these clusters can now be treated a 
different sub-matrix itself $A_1, \dots, A_i$ of the original matrix $A$.

Designing the algorithm it should require a $n \times m$ binary matrix A. And the two parameters $r > 0$ limiting the
maximum number of clusters, and $k \geq 0$ limiting the cost of all total distances.

Firstly all the vectors have to be constructed into clusters of equal vectors denoted by a new set $I^* = \{I_1, \dots, I_t\}$, henceforth 
called the \textit{initial clusters}; subsequently limiting the size of each cluster \textit{s.t.} $|I_i| \leq k+2$. This should
fulfil\mref{red:equalclusters}. Also note that if $t > k+r$ we already have no solution so return NO.

Secondly the algorithm should group all vectors of distance $d_H(u,v) \leq k$ into the same clusters as of\mref{red:distmaxk}. This can be done greedily from any
arbitrary vector. While performing this task, if at any point, the algorithm produces more than $r$ clusters, then return NO.

Lastly the algorithm should remove all uniform rows from any cluster $|J| \geq 2$ as of\mref{red:uniformrow}
\input{algorithms/kernel.tex}

\subsection{Dynamic programming}
The original r-means kernelization algorithm by \cite{fomin_golovach_panolan_2020}
intended for the sub-matrices $A_i=A_1,\dots,A_s$ to be stitched together to a single
matrix before running the r-means algorithm. However as of \cite[Lemma 5]{fomin_golovach_panolan_2020}
then instead of stitching the solution into a single matrix $A\sp{\prime}$
we can run the algorithm multiple times over $A_i$ with increasing parameter $k+r$,
save the different possible solutions, and stitch the solutions as the end. Thus saving
memory space and allowing the sub-matrices to be run in parallel.

This reconstruction is done dynamically by constructing a set of possible solutions $S_L$
\input{algorithms/solution-reconstruction.tex}

\subsection{Branching Algorithm}
Given a $n \times m$ binary matrix $A$, we should be able to find its centres. Due to the kernel \mref{alg:kernel}, the instances to be run, should all be as small as possible, and
close in distance. Still, we will need to test each one with a $k$ and $r$ parameter. To find the best centres is a complex algorithm, requiring combinatorics which
we can do with branching. Since we used $k$ to denote the max cost in the kernel \mref{alg:kernel}, and $k$ denotes the 
maximum cost also for the entire run from start to finish, we will here use $d$ to denote the maximum cost in 
any single run of\mref{alg:main}. The following algorithm is based on the works of \cite{fomin_golovach_panolan_2020}, but slightly modified to reduce implementation complexity. It's a recursive 
branching algorithm, which can be divided into several parts.

Beginning the algorithm we should define the starting parameters, as we will use a set 
$I \subseteq \{1, \dots ,n\}$ to define the remaining vectors to be tested by the algorithm, starting as the complete list
from $1, \dots ,n$ of the matrix $A$. We will need a set of the final vectors $S = \left[ x ~| ~x \subseteq \{0,1\}^m \right]$
of size at most $r$, which will be initialized as $S = \emptyset$. And finally the non-negative integer $d$.

For each iteration, we must do the basics of many recursive functions and test if we have a 
solution, or a non solution before we attempt to continue on the branch.

Then for each value from $0$ up to $d$ we must try all possible values, denoted by $h$, as the main part 
of\mref{alg:main}. This can again be split into three parts; the first of which is a linear clean up, where 
we can remove any vectors from the processing if they are now already covered by a selected center, within
a distance of $h-1$.

Now, depending on the remaining cost, and unprocessed binary vectors, we must either brute-force a solution
of branch further. This is denoted by the formula
\[
    |I| \leq \sqrt{d r ~log(w) ~/ ~log(r)}    
\]

For brute forcing, we simply try all possible partitions $\{J_0, \dots ,J_p\}$ of $I$ from $p=1$ to either
the length or the remaining binary vectors ($|I|$) or the remaining allowed centres ($r-|S|$). \textit{(Here $J_0$ may
also be empty)}. Then we simply find the optimal means $s^j$ for each cluster $J_j$ by using the majority rule. Recall that 
the majority rule is simply the most prevalent bit at each position of the vectors.

For branching, we will branch on all permutations of the vector $s \in \{0,1\}^m$ where the vector $s$ has a
distance of $h$ to another vector $a^i$ where $i \in I$. This vector $s$ will then be a new centre so we recurse
with $S = S \cup \{s\}$.

If no solution has then been found, return NO.

\input{algorithms/main-branching.tex}

% % % % % % % % % % % % % % % % %
%
%	4. Implementation
%
\newpage

\section{Algorithm implementation}
\begin{itemize}
    \item No kernel stitching
    \item No "agrees with"
\end{itemize}
\subsection{Tools used}
\subsubsection{The Java programming language, Version 17}
Selecting the programming language for any project is always an issue, and the choice of using
the Java programming language was mostly based on availability and the existing operations
and packages in the language, rather than the runtime.
\\
The Java programming language is not as fast as many of its alternatives, but does run almost
seamlessly on any operating system due to running on the JVM (Java virtual machine).

\subsubsection{GCP / BigQuery}
\label{sec:bigquery}
Running several thousand of tests multiple times and storing all results to a database with
input and resulting output can easily fill large amounts of data. To avoid bottlenecking the
testing computer all results could be pushed to an external database. For this the cloud database
"BigQuery" in the "Google Cloud Platform" could be used.

\subsubsection{Google Data studio}
\label{sec:datastudio}
The advantage of using an external database solution like BigQuery is that it's also easily
integrated in other tools. "Google Data studio" can create statistical plots based on large
data systems, and is able to directly query the BigQuery database for almost live updates.

\subsubsection{JetBrains - IntelliJ IDEA}
For any development in a high-level programming language an intelligent development environment
(IDE) is usually recommended. The IntelliJ IDEA is a premium alternative of many IDEs.

\subsubsection{Git / GitHub}
During development of an application it's recommended to use a version-control tool. GIT is
one of the most known version-control-tools in the field, and allows the user to traverse
the entire solutions history at any time.


To use GIT as a tool, an external GIT hosting solution can improve the experience while acting
as a backup tool. While a simple server could be sufficient, advanced solutions like GitHub
exist to add extra functionality.

\subsubsection{Maven}
Maven is a package manager for the Java Programming language, allowing the user to easily
list the needed dependencies of their project, then handling the rest.

\subsubsection{Travis}
Travis is a build tool that can seamlessly integrate with most Git solutions such as GitHub
to automatically fetch the newest version of any repository, and run build scripts defined
within the project. It also allows to run unit-tests and push the report.

\subsubsection{Codacy}
Codacy is a code quality evaluation tool, that can seamlessly integrate with most Git
solutions such as GitHub to automatically fetch the newest version of any repository,
and run a quality control on the code to catch any typical errors, bad habits, or
simply code that breaks standards to enhance readability and further development.


Codacy also holds a test-report functionality, though it lacks the ability to compile
and run any code or tests, it allows other tools to push the test-results in such that
in can serve as a total quality-control dashboard for the project.

\subsubsection{Combinatoricslib3}
Combinatoricslib3 is a open source Java library available on GitHub with existing
combinatoric functions such as partitioning.

\subsubsection{Testing computers}
\label{sec:computer}
The algorithm tests were run on a two testing computers
\begin{itemize}
    \item "Lenovo YOGA 900" with "Intel Core i5 (6. gen) 6200U / 2.3 GHz"
    8GB (1600 MHz / PC3-12800), LPDDR3 SDRAM. 256GB SSD M.2 memory, Intel HD Graphics 520.
    Running the minimal operating system, Linux distribution ArchLinux.
    \item A custom built desktop with "AMD FX-8350", "16.0GB Dual-Channel DDR3 @ 722MHz" memory, and a
    4095MB NVIDIA GeForce GTX 980 graphics card. Running the more standardized operating 
    system "Microsoft Windows 10".
\end{itemize}

\subsection{Data structures}
\subsubsection{BinaryVector}
Each binary vector is represented as a BitSet defined in the Java programming language. A
BitSet could be defined as a set of positions which is set to $1$. Hence the binary byte
representation of the number $42$ could be represented as the set
$42 \equiv 00101010 \equiv (3,5,7)$. The Java library version of BitSet also includes several
useful binary operations and extras such as cardinality. It should though be noted that the
BitSet standard is not Thread-safe \cite{bitset_java_14_2020}


An alternative implementation could be to use numbers to store the bits and use the standard
bit-operators of any programming language. Unfortunately this creates a limit in the length
of the vectors and their mutability which would require workarounds, but could theoretically
improve the runtime.

\subsubsection{BinaryMatrix}
The main Binary matrix structure is defined as a list of vectors or BitSets, along with the
original height and width.

\subsubsection{BinarySubMatrix}
The binary sub matrix is mainly copies of a binary matrix, and along also stores the parent
binary matrix reference. This allows for deletion of rows and columns, for deleted rows and
columns to be restored, and to find original positions in the parent matrix.

% % % % % % % % % % % % % % % % %
%
%	5. Testing
%
\newpage

\section{Testing}
\subsection{Generating test matrix}
For the generation of a matrix used for testing we could use any complete randomized binary
matrix, but to be able to control the result after the completion of the algorithm it is best
to create more concentrated tests.

\input{algorithms/test-generation.tex}

By reversing the algorithm we can generate a set of vectors $H$, which will be the centres,
and by picking random vectors from $H$ we will create a matrix with a solution of $K=0$,
and $R=H$. By then flipping random bits we can slightly modify the expected result, mainly
the K parameter. By flipping $D$ bits in the matrix we will still have a solvable matrix
with $R = H$ as long as $K \geq D$ though the matrix might still be solvable within both
smaller $K$ and $R$.


Still for normal runtime analysis the importance is to test the worst cases for the algorithm,
and while the concentrated tests is controllable, most of those will be mostly solved by the
kernel, not allowing for proper testing of the actual algorithm. For this the generation
algorithm would run with small parameters ($R \leq 5$, and $K \leq 30$).

\subsection{Running the test}
The algorithm testing was performed on randomly generated $1000 \times 1000$ matrices, on the computer
specified \mref{sec:computer}. Every single iteration would generate a new matrix, then measure the
milliseconds of the single iteration, and push the runtime with metadata to the cloud database specified
in \mref{sec:bigquery}.

The data for each iteration is its runtime, but metadata was added to separate later for analysis. The
metadata added was all the input parameters as $k,r,d,h, width, heigh$, the solution results, whether it was solved, the
computer running the test (in case of multiple test machines), and the date-time of the test case.

\subsection{Analysis}
To analyse the results of the several thousands of tests, a statistical tool was used, specified at
\mref{sec:datastudio}. In the tool the database was imported directly as a table, and could produce
plots to visualize the data. 

In Data studio, new columns were added to support a multi-parameter plot for better comparison of the
algorithms behaviour over the different parameters.

\subsection{Results}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/Binary_matrix_runtime.pdf}
    \caption{Average runtime in ms, filter: Only solved problems, $H$ being the number of centres generated in test, $1 \leq d \leq 2k+1$, width and height = 1000, $k$ is initial $k$ limit, and $r$ is initial $r$ limit.}
    \label{fig:result-table}
\end{figure}

\subsection{Testing conclusions (Brief 1 paragraph)}

% % % % % % % % % % % % % % % % %
%
%	6. Conclusion
%
\newpage

\section{Conclusion}
\subsection{Results}
\subsection{Future work}
\begin{itemize}
    \item Multithreading
    \item Better data structure and binary operations
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% % % % % % % % % % % % % % % % %
%
%	ADD REFERENCES
%
\newpage
\bibliography{extras/citation-db}
\bibliographystyle{apalike} %unsrt
\addcontentsline{toc}{section}{References}

\end{document}
