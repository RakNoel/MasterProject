% !TeX root = thesis.tex

%	Oskar L F Leirv√•g
%	Masters Thesis

\documentclass[a4paper]{article}
\input{extras/packages.tex}
\input{extras/commands.tex}

\begin{document}

% % % % % % % % % % % % % % % % %
%
%	FRONT PAGE
%
\input{extras/frontpage.tex}

% % % % % % % % % % % % % % % % %
%
%	Acknowledgements
%
\section*{Acknowledgements}
empty
\newpage

% % % % % % % % % % % % % % % % %
%
%	0. Abstract
%
\section*{Abstract}
In the \cite{fomin_golovach_panolan_2020} paper, the authors gave an exact parameterized
algorithm for the Binary r-Means clustering problem, parameterized by $k+r$, with the runtime of
$2^{\mathcal{O} (\sqrt{rk log(k+r) logr})}*nm$. The problem of Binary r-Means takes as input
an $n \times m$ binary matrix \textbf{A} with columns ($\textbf{a}^1,...,\textbf{a}^n$), a positive
integer $\textbf{r}$ and a nonnegative integer $\textbf{k}$.
The task is then to decide whether there is a positive integer $\textbf{r}\sp{\prime} \leq \textbf{r}$,
a partition $\{I_1, ..., I_{r\sp{\prime}}\}$ of $\{1,...,n\}$ and vectors
$(\textbf{c}^1,...,\textbf{c}^{r\sp{\prime}}) \in \{0,1\}^m$ such that
$\sum_{i = 1}^{r\sp{\prime}} \sum_{j \in I_i} d_H(c^i, a^j) \leq k $.

As the Binary r-Means problem is in the NP space, no other viable exact algorithm is currently known. Still due to
the nature of NP-algorithms complexity, its performance as a practical implementation is not yet known.
A proof of concept version of the algorithm was therefore implemented and tested against randomly generated binary matrices,
as to measure its performance on a standard desk-top computer.

The results show that even a sub-optimal implementation is inconceivably better than a brute force, and
viable with small parameters, while for data-mining greater resources and time could open for more use cases. More
research is needed, but the results are promising.


\newpage

% % % % % % % % % % % % % % % % %
%
%	TABLE OF CONTENTS
%
\pdfbookmark{\contentsname}{toc}
\tableofcontents
\newpage

% % % % % % % % % % % % % % % % %
%
%	TABLE OF ALGORITHMS
%
\addcontentsline{toc}{section}{List of Algorithms}
\listofalgorithms
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % %
%
%	1. Introduction
%
\section{Introduction}
\label{sec:intro}
In the current age of technology, humans have gathered unfathomable amounts of data wherever
we saw the possibility to log something. Ranging from major corporations to single individuals,
as long as we do anything digital, data is being logged, and the amount can only be estimated to be
up in Exabytes.
It's well known that there is useful information not only in the direct statistics of this data,
but in the anomalies, patterns and correlations between the data. The process of looking though and
comparing all this data, has become known popularly as Data mining, due to the complexity of the work,
and time required.

One technique of many to find correlations between data in big datasets is clustering. While
clustering, we in general look on data-points and organize the data-points
into sets; where any data-point within a set, is more similar to all other data-points inside
the same set, than all other data-points in other sets. Hence by adding any new
data-point, we should be able to find which cluster or classification-set it is closest too,
thereby also classifying it. \cite{gan07}

Now suppose we have a streaming webpage that collects data on whether a user like any given movie or not,
then we might want to use this data to recommend movies to any specific user which they might like.
If we can classify our users into groups that share mostly the same taste in movies, then
the "average-user" or centres of the classifications would be a "super user" most likely to share the same movie
interest as any other user in the same classification.

To quickly group the new users, we can use these averages or centres. This is called centroid-based
clustering, so that all data-points can be linearly classified by finding it's closest centre.
The problem then becomes defining what is a centre, and how do we find them.

Unfortunately we now know that to cluster and create these centres accurately is an NP-hard problem,
and to create such centres for small instances might not be viable even with massive computing power,
over long time frames.

\subsection{Motivation}
Much research has gone into developing theoretical algorithms for the most complex computational
problems in computer science, however many of them remain purely theoretical, not because
they're not useful, but because the implementation of brute-force algorithms is simpler, and
computational power can be bought. Testing the viability and practicality of implementing
these complex algorithms is an important step in the research done for improving the
algorithms, and the many practical limitations we face while constructing them in our
preferred languages.

This paper aims to clarify the issues faced during development, and the results one could expect
on a practical level of computation time on a smaller home computer/laptop.

\subsection{Problem statement}
From example above with our streaming webpage, we get a rather unique situation where the problem consist
purely of binary data-points. We want to find a viable centroid-based clustering algorithm, to find
centres in clusters of binary vectors, in a reasonable runtime.

For this paper, we will look into a theoretical centroid-based binary vector clustering algorithm, proposed
by \cite{fomin_golovach_panolan_2020}. First we need to find a distance metric between the vectors so we
can group them. Because we're clustering binary vectors/matrices, the standard clustering metric is
the Hamming distance (\textit{$d_H(a, b)$}) between the binary vectors. We will here represent the input of these
vectors as a single matrix, where each column is a binary vector.

This proposed theoretical algorithm is a parameterized algorithm, limiting the runtime of the
algorithm parameterized by the two parameters $k$ and $r$, where $k$ is the total maximum deviancy
of all data-points to their respective centres, and $r$ is the maximum allowed clusters and
thereby total centres allowed in a solution.

Now lets look back to our example. We want to find the "centres" or "super users" which we can use to
give new movie recommendations to new users. Let's see how we have to encode this problem towards
a more formal language.

To convert this problem to a binary matrix, we can define each individual user as a vector
$u=\{0,1\}^m$ where the individual bit-number in the vector represent the same movie for all users
and indicates whether the user liked ('1') or disliked the movie ('0'), such that if the two
user-vectors $u=\{1,1,\ldots\}^m$ and $v=\{0,1,\ldots\}^m$ we say that $u$ and $v$ both like movie $2$,
but disagree on movie $1$ as $u_1 \neq v_1$.

We then transform the vectors and place them in the set $U^n$. Now we can use the user-vectors of $U$
as columns in a matrix such that
\[
    A(U)  \rightarrow \begin{pmatrix}
        1       & 0       & \cdots & a_{1,n} \\
        1       & 1       & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
\]

When running the algorithm we also have to select the parameters $k$ and $r$ where $r$ would be
how many centres of movie-personalities we wish to define, and $k$ how much the users in
the training-set would be allowed to deviate from the centre in total, also known as the cost.

The resulting vectors would now be the new set of super-users to use in a recommendation system,
linearly comparing every new user to the one centre they are closest to whenever they rate movies.

Now let's formally define the problem then, of "Binary r-Means".
\begin{problem}[Binary r-Means]
\begin{tabular}{p{0.1\textwidth}p{0.8\textwidth}}
    \textit{Input}: & An $n \times m$ binary matrix \textbf{A} with columns
    ($\textbf{a}^1,...,\textbf{a}^n$), a positive integer $\textbf{r}$ and a nonnegative
    integer $\textbf{k}$                                                                     \\

    \textit{Task}:  & Decide whether there is a positive integer $\textbf{r}\sp{\prime} \leq
        \textbf{r}$, a partition $\{I_1, ..., I_{r\sp{\prime}}\}$ of indices $\{1,...,n\}$ and vectors
    $(\textbf{c}^1,...,\textbf{c}^{r\sp{\prime}}) \in \{0,1\}^m$ such that

    \[
        \sum_{i = 1}^{r\sp{\prime}} \sum_{j \in I_i} d_H(c^i, a^j) \leq k
    \]
\end{tabular}
\end{problem}

Now given any set $S$, it is trivial to find the centre of that set in polynomial time. The challenge
here is that we don't know the sets, and we need a centre to create the sets. It simply becomes a problem
of which comes first, the chicken or the egg.

Because we need to know the sets, before we can find the centres in the algorithm, we will
generally need to compare all data-points to all other data-points, creating a $\mathcal{O}(n!)$ runtime. This
is highly undesirable as only a few thousand vectors could result in a runtime longer than the lifetime
of the universe. The algorithm from the paper \cite{fomin_golovach_panolan_2020} has however proposed a
runtime of $2^{\mathcal{O} (\sqrt{rk log(k+r) logr})}*nm$; placing the algorithm in the FPT class, which is
much closer to the runtime we might expect from normal computers.

\subsection{Previous results}
to be done

\subsection{Overview}
\Mref{sec:intro} has now presented the paper, and given an introduction to the main problem at hand and the
motivation behind it. In \mref{sec:prelim} the most basic notations and definitions which will be used in
later sections are stated. Then \mref{sec:algo} introduces the algorithms implementation in three main parts, and
explains how they work in some detail. \Mref{sec:impl} presents all the tools used in this paper, and states which
custom objects were defined and used in the actual implementation. After that \mref{sec:testing} details how the
testing was designed and run, how the data was stored, processed and analysed, before detailing the results. Finally
in \mref{sec:conclusion} we have the conclusion, and a suggestion for further work.

% % % % % % % % % % % % % % % % %
%
%	2. Preliminaries
%

\newpage

\section{Preliminaries}
\label{sec:prelim}
\subsection{Notations and Definitions}
\subsubsection{Matrices}
As we are clustering binary vectors $v \in \{0,1\}^m$, we can transform any set of vectors
$V$ into a matrix $A(v^1, v^2, \cdots, v^n)$ where $n$ is the number of columns
and $m$ is the length of the vectors.

\[
    A_{m,n} =
    \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
\]

\subsubsection{Hamming distance}
For clustering we need to compare the columns of matrices by distance, to calculate the distance
between columns we will use the hamming distance. Let us recall that the hamming distance
between two binary vectors $x,y \in \{0,1\}^m$ where $x=(x_1,...,x_m)^T$ and
$y=(y_1,...,y_m)^T$, is

\[
    d_H(x,y)= \sum_{i = 1}^{m} |x_i - y_i|
\]

or, in other words, the number of positions $i \in \{1,...,m\}$ where $x_i$ and $y_i$ differ.

With that distance we can
also define the distance between two matrices as the sum of the distance between the columns of
the matrices. Now we can also define the cardinality of a binary vector as the number of positive bits in
the vector $v \in \{0,1\}^m$ such that $d_C(v)=\sum_{i = 1}^{m} v^i$.

\[
    d_H(A,B) = \sum_{i = 1}^{n} d_H(a^i, b^i)
\]

\subsection{Majority rule}
To find a single optimal binary centre $s \in \{0,1\}$ in a set of binary columns $I$, it must follow, that for each row,
the bit in $s_i$ must be the same value as the most prevalent bit in the row $I_i$.

\subsection{Parameterized algorithms}
As the main algorithm in this thesis is based on a parameterized problem, we here refer to the
book \cite{param_algo_book} for a detailed introduction. Here we shall only introduce the most
basic notions.

\begin{theoremdefinition}[Parameterized problem]{\cite[p.~12]{param_algo_book}}
    A \textit{parameterized problem} is a language $L \subseteq \Sigma^* \times \mathbb{N}$,
    where $\Sigma$ is a fixed, finite alphabet. For an instance $(x,k) \in \Sigma^* \times \mathbb{N}$,
    $k$ is called the \textit{parameter}.
\end{theoremdefinition}

With the problem of r-means from \cite{fomin_golovach_panolan_2020}, we here consider it as such
a parameterized problem, with $(A,k+r)$

\begin{theoremdefinition}[Fixed-parameter tractable]{\cite[p.~13]{param_algo_book}}
    A parameterized problem $L \subseteq \Sigma^* \times \mathbb{N}$ is called
    \textit{fixed-parameter tractable} (FPT) if there exists an algorithm $\mathcal{A}$ (called
    a \textit{fixed-parameter algorithm}), a computable function $f:\mathbb{N} \rightarrow \mathbb{N}$,
    and a constant $c$ such that, given $(x,k) \in \Sigma^* \times \mathbb{N}$, the algorithm
    $\mathcal{A}$ correctly decides whether $(x,k) \in L$ in time bounded by $f(k) * |(x,k)|^c$.
    The complexity class containing all fixed-parameter tractable problems is called FPT
\end{theoremdefinition}

By this definition the proposed algorithm from \cite{fomin_golovach_panolan_2020} should have
a runtime in the form of $f(k) * |(x,k)|^c$. We can see this in the proposed runtime which is
proposed to be $2^{\mathcal{O} (\sqrt{rk log(k+r) logr})}*nm$.

\begin{theoremdefinition}[Data reduction rule]{\cite[p.~18]{param_algo_book}}
    A \textit{data reduction rule}, of a parameterized problem $Q$ is a function
    $\phi \Sigma^* \times \mathbb{N} \rightarrow \Sigma^* \times \mathbb{N}$
    that maps an instance $(I,k)$ of $Q$ to an equivalent
    instance $(I^\prime, k^\prime)$ of $Q$ such that $\phi$ is computable in polynomial time
    in $|I|$ and $k$. We say that two instances of $Q$ are \textit{equivalent} if the following
    holds: $(I,k) \in Q$ if and only if $(I^\prime, k^\prime) \in Q$.
\end{theoremdefinition}

\begin{theoremdefinition}[Kernelization, kernel]{\cite[p.~18]{param_algo_book} \label{def:kernelization}}
    A kernelization algorithm, or simply a kernel, for a parameterized problem $Q$
    is an algorithm $\mathcal{A}$ that, given an instance $(I,k)$ of $Q$, works in polynomial
    time and returns an equivalent instance $(I^\prime, k^\prime)$ of $Q$. Moreover, we require
    that $size_{\mathcal{A}}(k) \leq g(k)$ for some computable function
    $g: \mathbb{N} \rightarrow \mathbb{N}$.
\end{theoremdefinition}

\begin{theoremdefinition}[Turing kernelization]{\cite[p.~314]{param_algo_book} \label{def:turing-kernelization}}
    Let $Q$ be a parameterized problem and let $f:\mathbb{N} \rightarrow \mathbb{N} $
    be a computable function. A \textit{Turing kernelization} for $Q$ of size $f$ is an
    algorithm that decides whether a given instance $(x,k) \in \Sigma^* \times \mathbb{N}$
    is contained in $Q$ in time polynomial in $|x|+k$, when given access to an oracle
    that decides membership in $Q$ for any instance $(x^\prime, k^\prime)$ with
    $|x^\prime|,k^\prime \leq f(k)$ in a single step.
\end{theoremdefinition}

% % % % % % % % % % % % % % % % %
%
%	3. Algorithm
%

\newpage

\section{Algorithm}
\label{sec:algo}
\subsection{Kernelization}
\label{sec:algo:kernel}
The goal of the preprocessing algorithm is to reduce the input matrix by following certain reduction steps
based on the two parameters $k$ and $r$, running in polynomial time over the input size, here ($n \times m$).

Designing the algorithm it should require an $n \times m$ binary matrix A. And the two parameters $r > 0$ limiting the
maximum number of clusters, and $k \geq 0$ limiting the cost of all total distances. By \mref{def:kernelization}, it must
produce an equivalent problem, such that we should expect an output of a binary matrix $A^\prime$ and the parameters $k^\prime$
and $r^\prime$.

The first observation is that if there are more than $k+r$ distant columns, there can not be a solution, so we can simply return NO.
Therefore as a start we partition the columns into clusters of equal columns $I = {I_1, \dots, I_t}$ and test if $t \ge k+r$.
We will call this partitioning $I$ and its groups for the \textit{initial clusters}. Further we can assume that all initial
clusters will be clustered together, so if the size of such an initial cluster $|I_i|$ is larger than or equal to $k+1$,
we assume that there must be a centre with zero distance to this cluster. Thus, any initial cluster of size $|I| \geq k+2$
can be reduced down to $|I| = k+1$.

Now, we can greedily group the initial clusters together, with any other cluster of distance $d_H(I_a,I_b) \leq k$ and call this new
partition $L$. From the way we constructed these clusters, we now have that the distance between any two columns from any to separate
clusters, is of a distance $d_H > k$, and cannot be clustered together within the cost of $k$.

Instead of treating this as a single solution, we can process each of the clusters individually as its own matrix, henceforth
a \textit{sub-matrix}. Now we can remove what we can call \textit{uniform rows}. A uniform row, is a row consisting
only of zeros or only ones. If we remove a uniform row, it will not have any impact on the final result as the optimal centre
should keep the uniformity of the row for zero cost. Since all columns were clustered together with those of distance $d_h \leq k$,
the number of pairwise distant columns is at most $k+r$, and we can thus conclude that the number of non-uniform rows is bound.

All this is in fact explained by the following lemma
\begin{theoremlemma}{\cite[Lemma 5]{fomin_golovach_panolan_2020}}
    Given an instance $(A,r,k)$ of Binary r-Means, Algorithm 1 runs in $O(n^2m)$ time
    and either correctly concludes that $(A,r,k)$ is a no-instance of Binary r-Means and
    returns NO or returns $s \leq r$ matrices $A_1, \dots, A_s$ such that
    \begin{itemize}
        \item $(A,r,k)$ is a yes-instance of Binary r-Means if and only if there are positive
              $r_1,\dots,r_s$ and nonnegative $k_1,\dots,k_s$ such that $(i)$ $r_1 + \dots + r_s \leq r$,
              $(ii)$ $k_1 + \dots + k_s \leq k$, and $(iii)$ $(A_i,r_i,k_i)$ is a yes-instance of Binary
              r-Means for every $i\in\{1,\dots,s\}$
        \item for every $i\in\{1,\dots,s\}$, $A_i$ is $m_i \times n_i$ matrix with
              $m_i \leq max\{(\ell_i - 1)k, 1\}$, where $\ell_i$ is the number of distinct columns
              of $A_i$ and $n_1 + \dots + n_s \leq (k+1)(k+r)$
        \item the total number of distinct columns in $A_1,\dots,A_s$ is at most $k+r$
    \end{itemize}
    \label{lem:5}
\end{theoremlemma}

\input{algorithms/kernel.tex}

When reducing the initial matrix, the algorithm has now produced a list of sub-matrices $L$, which might seem to conflict with our expectations
of $\mathcal{A}(A,k+r) \rightarrow (A^\prime,k^\prime+r^\prime)$. However, from \mref{lem:5} we know that a combination of the solutions of the
individual sub-matrices in $L$ should produce a solution for the initial binary matrix $A$, thus we actually have a \mref{def:turing-kernelization}.
The papers original pseudo-code \cite{fomin_golovach_panolan_2020} gives an algorithm to create a substitute matrix "gluing" together these clusters into
one single matrix, but as they can be processed individually, we will do so instead.

\subsection{Branching Algorithm}
\label{sec:algo:branching}
Now from the kernel in \mref{alg:kernel} we are given a list $L$ of sub-matrices, which can be processed individually. Here we
shall look at the main step of the algorithm; the Binary r-Means, recursive branching algorithm.

We should have an input in the form of a binary matrix $A$, or as we will only reference its columns, a set
$I \subset \{1,\dots,n\}$, assuming we can reference $A$ and any column $A^n$. We also need the parameters $k$ and $r$.
Since it is a recursive algorithm, we also need to send the partial solution, which will here be the set of selected
centres $S$. Since we do not have any selected centre, the initial value will therefore be the empty set for
$S := \emptyset$. Also, we should expect to see a change in the parameter $k$, so we will rename it to $d$ to represent the
current available cost, with the initial value $d := k$

For each recursion, we must check weather the branch has either produced a solution, or exceeded the parameters allowed. We know that
we have a set of selected centres $S$, so if $S$ can cover all remaining columns within the cost $d$ we have the solution and return $S$.
If however we have $|S| = r$ then we have a NO-instance and return.

If the algorithm can continue, we can conclude that the optimal solution given the set $S$ must have at least one more element. Thus
we try to find centres of minimum distance less than $d$ to another column. We shall here call the currently selected minimum
distance $h$. We start from zero, and work $h$ up until $d$. Now any remaining columns, whose distance to any centre in $S$,
is less then $h$, will be closer to a column currently in $S$ than a new centre, and can therefore be covered by $S$,
removed from $I$, while adjusting the remaining budget $d$ appropriately.

Now, to find the new centre we can either branch or brute force. The efficiency of which is determined by the formula
\[
    |I| \leq \sqrt{d r ~log(m) ~/ ~log(r)}
\]

If we can brute force, we try all partitions of $I$ with size $p$ up to either $|I|$ or the remaining budget of $r$, whichever comes first. Then
for all clusters $j$ in a partition $J_j$ we find the optimal means $s^j$ for the cluster, by using the \textit{majority rule}. And test if it produces
a solution when adding $\{s^1,\dots,s^p\}$. If so, return $S$

If however we cannot brute force, then we must branch. We must try all possible vectors $s \in \{0,1\}^m$ whose distance to a remaining column in $I$
is equal to $h$. We iterate over all the vectors $s$, and recurse by calling r-Means with $S= S\cup\{s\}$. If we find a solution then return it.
\textit{Note: the original algorithm paper \cite{fomin_golovach_panolan_2020} specified that the branching should be on permutations using "Agrees with"
    to limit branching slightly further, which was here skipped for practical reasons.}

If no solution is found, we must increase the minimum cost $h$. Though, if $h$ is no longer less than $d$ there cannot be any solution, and we return NO.

\input{algorithms/main-branching.tex}

With a correct implementation this algorithm should produce and return a new set $S$ containing the new proposed
centres for each cluster. The sum of distances from any column in $A$ to the closest center in $S$ should then be
less than or equal to $d$, and the number of centres in $S$ should be less than or equal to $r$.

\subsection{Dynamic programming}
\label{sec:algo:dynamic-result}
The original r-means kernelization algorithm by \cite{fomin_golovach_panolan_2020}
intended for the sub-matrices $L=A_1,\dots,A_s$ produced from the kernel in \mref{sec:algo:kernel} to be \textit{"glued"} together
into a single matrix before running the r-means algorithm. However as of \mref{lem:5} then instead of stitching the solution
into a single matrix $A^\prime$ we can run the algorithm multiple times over $A_i$ with increasing parameter $k$,
save the different possible solutions in a set $H$ \textit{(hypotheses)}, and stitch the solutions in the end. Thus saving
memory space and allowing the sub-matrices to be run in parallel.

This reconstruction can be described programmatically, but is better described functionally. We define the function $f(k)$ to find
the a solution of

\[ f(k) =
    \begin{cases}
        \text{NO - Instance}  & \quad \text{if } (r_1 + \hdots + r_s \geq r+1 ~|~ k_1, \hdots, k_s \leq k) \\
        \text{YES - Instance} & \quad \text{if } k \leq r
    \end{cases}
\]

As for every $i \in \{1,\dots,s\}$ we have the cases
\[ f_i(k^\prime) =
    \begin{cases}
        \text{min \#clusters that can be obtained for} A_i \text{ using at most } k^\prime &                                \\
        r+1,                                                                               & \quad \text{if \#clusters} > r
    \end{cases}
\]

We have in \mref{sec:algo:branching} three given the algorithm for finding this with the parameters, hence
\[
    f_i(k^\prime) = \text{Binary r-Means}(I=\{1,\dots, A_{i_n}\}, S = \emptyset, r^\prime, k^\prime).
\]

Now as equal calls will be made multiple times, we can store the values in a table, and even better, we can store the minimum \#clusters we can achieve in all clusters up to $i$.
\[
    T[i, k^\prime] \rightarrow \text{min \#clusters that can be obtained for the set } \{A_1, \dots, A_i\}, \text{ using at most } k^\prime
\]

which we can define with the functions $f_i(k^\prime)$ by
\[
    \begin{tabular}{lcl}
        $T[1, k^\prime]$ & $=$ & $f_1(k^\prime)$                                                                                              \\
        $T[i, k^\prime]$ & $=$ & $min\{ min\{ T[ i-1, k_1 ] + f_i(k_2) ~|~ k_1 + k_2 = k^\prime \text{~and~} k_1 \wedge k_2 \geq 0\}, r+1 \}$
    \end{tabular}
\]

With which we can simplify $f(k)$ into
\[ f(k) =
    \begin{cases}
        \text{NO - Instance}  & \quad \text{if } T[s,k] \geq r+1 \\
        \text{YES - Instance} & \quad \text{if } T[s,k] \leq r
    \end{cases}
\]

\begin{proof}
    As $r^\prime$ is a sub-solution, we can define it as $r^\prime = T[i, k^\prime]$, such that $r^\prime$ is the min \#clusters
    that can be obtained for the set $\{A_1, \dots, A_i\}$, using at most $k^\prime$. This creates an inequality, as we know that
    $T[i, k^\prime] \geq min\{\dots\}$.

    \[
        r^\prime = T[i, k^\prime] \geq min\{ T[ i-1, k_1 ] + f_i(k_2) ~|~ k_1 + k_2 = k^\prime \text{~and~} k_1 \wedge k_2 \geq 0\}
    \]

    now given the partial optimal solution $r^\prime$ produced within the cost $k^\prime$, if we divide both variables into two parts
    such that we have $r_1 + r_2 = r^\prime$ and $k_1 + k_2 = k^\prime$, then we can divide the partial problem in the same way
    $(A_1,\dots,A_{i-i} + A_i)$, and say that $r_1$ is the minimal \#clusters for $(A_1,\dots,A_{i-i})$ using at most $k_1$, and that $r^\prime$
    is the minimal \#clusters for $A_i$ by paying at most $k_2$

    \begin{tabular}{rl}
        $\rightarrow$ &                                                                                       \\
                      & Thus we have $r_1 \geq T[i-1,k_1]$ and $r_2 \geq f_i(k_2)$. And we re-state that as   \\
                      & $T[i,k^\prime] = r_1+r_2 \geq t[i-1,k_1]+f_i(k_2) \geq min\{\dots\}$                  \\
        $\leftarrow$  &                                                                                       \\
                      & Now using the same assumption that $k_1 + k_2 = k^\prime$ and $r_1 + r_2 = r^\prime$, \\
                      & we will assume that there exists a solution $k_1,k_2$ such that                       \\
                      & $min\{\dots\} = T[i-1,k_1] + f_i(k_2)$ which we can restate as                        \\
                      & $T[i-1,k_1] + f_i(k_2) \geq r_1+r_2 | k_1+k_2 = k^\prime \geq T[i,k^\prime]$
    \end{tabular}
\end{proof}

\subsection{Summary}
Now lets summarize the complete algorithm. 

As first we call the pre-processing algorithm (kernel) in \mref{sec:algo:kernel} 
on the problem to hopefully reduce the input into smaller inputs $L$. 

Then given these instances $L = \{A_1, \dots, A_i\}$ we can find the minimum solution for any 
sub-matrix by calling the \textbf{Binary r-Means()} algorithm from \mref{sec:algo:branching}, from within the final 
dynamic algorithm in \mref{sec:algo:dynamic-result}. The dynamic programming algorithm allows us to store each result, avoid calling 
the hard method unnecessarily many times, also producing the result for the original initial binary matrix $A$

% % % % % % % % % % % % % % % % %
%
%	4. Implementation
%
\newpage

\section{Algorithm implementation}
\label{sec:impl}
\subsection{Tools used}
\subsubsection{The Java programming language, Version 17}
Selecting the programming language for any project is always an issue, and the choice of using
the Java programming language was mostly based on availability and the existing operations
and packages in the language, rather than the runtime.
\\
The Java programming language is not as fast as many of its alternatives, but does run almost
seamlessly on any operating system due to running on the JVM (Java virtual machine).

\subsubsection{GCP / BigQuery}
\label{sec:bigquery}
Running several thousand of tests multiple times and storing all results to a database with
input and resulting output can easily fill large amounts of data. To avoid a bottleneck the
testing computer all results could be pushed to an external database. For this the cloud database
"BigQuery" in the "Google Cloud Platform" could be used.

\subsubsection{Google Data studio}
\label{sec:datastudio}
The advantage of using an external database solution like BigQuery is that it's also easily
integrated in other tools. "Google Data studio" can create statistical plots based on large
data systems, and is able to directly query the BigQuery database for almost live updates.

\subsubsection{JetBrains - IntelliJ IDEA}
For any development in a high-level programming language an intelligent development environment
(IDE) is usually recommended. The IntelliJ IDEA is a premium alternative of many IDEs.

\subsubsection{Git / GitHub}
During development of an application it's recommended to use a version-control tool. GIT is
one of the most known version-control-tools in the field, and allows the user to traverse
the entire solutions history at any time.


To use GIT as a tool, an external GIT hosting solution can improve the experience while acting
as a backup tool. While a simple server could be sufficient, advanced solutions like GitHub
exist to add extra functionality.

\subsubsection{Maven}
Maven is a package manager for the Java Programming language, allowing the user to easily
list the needed dependencies of their project, then handling the rest.

\subsubsection{Travis}
Travis is a build tool that can seamlessly integrate with most Git solutions such as GitHub
to automatically fetch the newest version of any repository, and run build scripts defined
within the project. It also allows to run unit-tests and push the report.

\subsubsection{Codacy}
Codacy is a code quality evaluation tool, that can seamlessly integrate with most Git
solutions such as GitHub to automatically fetch the newest version of any repository,
and run a quality control on the code to catch any typical errors, bad habits, or
simply code that breaks standards to enhance readability and further development.


Codacy also holds a test-report functionality, though it lacks the ability to compile
and run any code or tests, it allows other tools to push the test-results in such that
in can serve as a total quality-control dashboard for the project.

\subsubsection{Combinatoricslib3}
Combinatoricslib3 is a open source Java library available on GitHub with existing
combinatorial functions such as generating permutations.

\subsubsection{Testing computers}
\label{sec:computer}
The algorithm tests were run on a two testing computers
\begin{itemize}
    \item "Lenovo YOGA 900" with "Intel Core i5 (6. gen) 6200U / 2.3 GHz"
          8GB (1600 MHz / PC3-12800), LPDDR3 SDRAM. 256GB SSD M.2 memory, Intel HD Graphics 520.
          Running the minimal operating system, Linux distribution ArchLinux.
    \item A custom built desktop with "AMD FX-8350", "16.0GB Dual-Channel DDR3 @ 722MHz" memory, and a
          4095MB NVIDIA GeForce GTX 980 graphics card. Running the more standardized operating
          system "Microsoft Windows 10".
\end{itemize}

Also see \mref{sec:future-work} for more information.

\subsection{Data structures}
\subsubsection{BinaryVector}
The first logical implementation would be to simply use a \textit{Boolean array}, which would
most likely work, but due to the implementation of the Boolean type in the Java Programming language,
it still uses 1byte \textit{1 Byte = 8 bits}, which is usually not a problem, but if we duplicate
a $1000 \times 1000$ bit array $1'000$ times, the size becomes \textit{1 GigaByte}. If we can create
a smarter solution using bits, the same matrices could be stored in one eight of the space
\textit{125 MegaBytes}

A solution for this is to use the bits directly, like using numbers to store the bits and use the standard
bit-operators of any programming language. Unfortunately this creates a limit in the length
of the vectors \textit{long = 64 bit} and their mutability which would require workarounds,
but could theoretically improve both the runtime, and the memory.

Luckily, the Java Programming language, standard library has an existing type whit a similar
implementation. Each binary vector is represented as a BitSet, which is not human-readable, but
a BitSet could be presented as a set of positions which is set to $1$. Hence the binary byte
representation of the number $42$ could be represented as the set
$42 \equiv 00101010 \equiv (3,5,7)$. The Java library version of BitSet also includes several
useful binary operations and extras such as cardinality. It should though be noted that the
BitSet standard is not Thread-safe \cite{bitset_java_17_2021}

\subsubsection{BinaryMatrix}
The main Binary matrix structure is defined as a list of vectors or BitSets, along with the
original height and width.

\subsubsection{BinarySubMatrix}
The binary sub matrix is mainly copies of a binary matrix, and along also stores the parent
binary matrix reference. This allows for deletion of rows and columns, for deleted rows and
columns to be restored, and to find original positions of all rows and columns in the parent
BinaryMatrix.

\subsubsection{BMAHypothesis}
The BinaryMatrixApproximationHypothesis objects, were created to structure the multiple
results produced iterating over all possible parameters. It is only a container for a
proposed solution to a BinarySubMatrix with a given parameter $d$, holding the used cost $d$
and the centres produced.

\subsubsection{Partition}
The partition object, was defined to extend on the Java standard library \textit{ArrayList$<$List$<$T$>>$}
structure and allow the creation of partitions from a list of objects.


% % % % % % % % % % % % % % % % %
%
%	5. Testing
%
\newpage

\section{Testing}
\label{sec:testing}
\subsection{Generating test matrix}
For the generation of a matrix used for testing we could use any complete randomized binary
matrix, but to be able to control the result after the completion of the algorithm it is best
to create more concentrated tests.

By reversing the entire algorithm we can generate a set of \textit{initial vectors} $H$ of length $h$,
which will be the centres, and by picking random vectors from the set $H$ into a new set $S$,
we will create a new random binary matrix $A$, with a solution of $k=0$, and $r=h$.

By then flipping random bits, we can slightly modify the expected result, mainly
the $k$ parameter. By flipping $d$ bits in $A$, we will still have a solvable matrix
with $r = h$ as long as $k \geq d$ though the matrix might still be solvable within both
smaller $k$ and $r$.

\input{algorithms/test-generation.tex}

Still for normal runtime analysis the importance is to test the worst cases for the algorithm,
and while the concentrated tests is controllable, most of those will be mostly solved by the
kernel, not allowing for proper testing of the actual algorithm. For this the generation
algorithm would run with small parameters ($r \leq 3$, and $k \leq 30$).

\subsection{Running the test}
\subsubsection{Setup}
The algorithm testing was performed on randomly generated matrices of varying sizes, on the computers
specified \mref{sec:computer}, and published at \cite{datastudio_results}.
Every single test iteration would generate a new matrix, then measure the
milliseconds of the single iteration, then push the runtime with metadata to the cloud database specified
in \mref{sec:bigquery}.

\subsubsection{Countermeasures}
The timing was measured directly inside the program, between generation and publishing as to only measure
the time used to solve, and discount any time used to generate or latency caused by slow connections. The tests
was allowed to run for extended periods, and mostly when the computers were not in use.

It was possible to run multiple tests simultaneously as the both computers is multi-core, making sure not to overload
the CPU usage.

\subsubsection{Cloud computing}
As to avoid the loss of control on the actual hardware running the tests, cloud "mining" was initially
avoided. As the testing showed to be very resource intensive however, it was theorized how it could be achieved.
Unfortunately many cloud providers have measures in place to avoid misuse in crypto-currency mining, and further
discouraged its use.

\subsubsection{Data collection}
The data for each iteration is its runtime, but metadata was added to separate later for analysis. The
metadata added was all the input parameters as $k,r,d,h, width, heigh$, the solution results, whether it was solved, the
computer running the test (in case of multiple test machines), and the date-time of the test case.

\subsection{Analysis}
To analyse the results of the several thousands of tests, a statistical tool was used, specified at
\mref{sec:datastudio}. In the tool the result-database was imported directly as a table, and plots were
created to visualize the data.

In Data studio, new columns were added to support a multi-parameter plot for better comparison of the
algorithms behaviour over the different parameters. This includes combining the variables $h$ and $r$ into
a unified $h, r$ variable such to illustrate the different runtimes in the same plots. This same technique was
also used to combine the running width and height to compare the runtimes of the matrix-size later.

\newpage
\subsection{Results}
First off, let us recall that we defined the variables as such
\[
    \begin{tabular}{lll}
        $h$ & $\rightarrow$ & \textit{\#initial clusters} used to generate the random binary matrices         \\
        $d$ & $\rightarrow$ & \textit{\#random-bit-flips} performed on the randomly generated binary matrices \\
        $r$ & $\rightarrow$ & the initial parameter $r$ that was given to the algorithm                       \\
        $k$ & $\rightarrow$ & the initial parameter $k$ that was given to the algorithm                       \\
    \end{tabular}
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, page=4]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $k$, using matrices generates of size Width: $1000$ and Height:$1000$.}
    \label{fig:res-1000x1000k}
\end{figure}

The first runs was done on several 1000x1000 binary matrices. We can see from the plot in \mref{fig:res-1000x1000k}
that the runtime is a function of both $k$ and $r$ as expected. We can also see that there is a large dependency of
on the amount of initial columns generated initially, which is also expected.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth, page=6]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $d$, using matrices generates of size Width: $1000$ and Height:$1000$.}
    \label{fig:res-1000x1000d}
\end{figure}

Looking at the same matrices over $d$ in \mref{fig:res-1000x1000d}, we can assume that the amount of noise
introduced to the initial matrices affected the runtimes. The noise variable $d$ is randomly dependent on $k$
but while we see from \mref{fig:res-1000x1000d} the average runtime of the worst case ($h:1,r:3$) at $k \simeq 30$
is somewhere around $150'000ms$ or 2 minutes and 30 seconds, the average runtime of the worst case keeps growing exponentially
to the worst case ($h:1,r:3$) at $d \simeq 60$ is somewhere around $600'000ms$ or 10 minutes.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth, page=3]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $k$, using matrices generates of size Width: $200$ and Height:$200$.}
    \label{fig:res-200x200k}
\end{figure}

The size of the matrix should affect the runtime so the algorithm should be tested on smaller matrices. The
following data is from 200x200 matrices. However looking at \mref{fig:res-200x200k} we see almost the same
results as we did in \mref{fig:res-1000x1000k}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth, page=7]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $k$, all data from all test-runs with all matrices from this report, grouped by
        the size of the matrices.}
    \label{fig:res-all-matrices-over-k}
\end{figure}

In fact almost all matrices seems to follow the same pattern of runtime with little to no effect on the runtime
between much larger matrices. This indicates that the kernel is able to mitigate the size efficiently, and that
the data-structures used does not severely impact the runtime of the results.

It is then to be expected that the rows are not a large factor of the runtime with a proper implementation,
hence the amount of columns and the distance between them has the greatest impact on the runtime, along
with the parameters $k$ and $r$.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, page=2]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $k$, using matrices generates of size Width: $100$ and Height:$20$.}
    \label{fig:res-100x20k}
\end{figure}

Now looking at \mref{fig:res-100x20k} we see different results. The runtime behaves quite differently from
both the \mref{fig:res-1000x1000k} and the \mref{fig:res-200x200k}. Here we can see what appear to be
three groups with similar runtime grouped by $r$. The three groups here are
\begin{itemize}
    \item $(h1,~r1),(h2,~r1),(h3,~r1)$
    \item $(h1,~r2),(h2,~r2),(h3,~r2)$
    \item $(h1,~r3),(h2,~r3),(h3,~r3)$
\end{itemize}

Compared to \mref{fig:res-200x200k} we can see that it starts off similar, but as $k$ grows, we get a
clear change in runtime. The $(h3,~r3)$ which seemed of no significance, now crosses over $(h1,~r3)$
with much higher average runtimes consistently. The same goes for $(h2,~r3)$, which for \mref{fig:res-200x200k}
was much lower.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, page=5]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $d$, using matrices generates of size Width: $100$ and Height:$20$.}
    \label{fig:res-100x20d}
\end{figure}

One theory is that as $k$ grows, so does the number of noise $d$, but looking at
\mref{fig:res-100x20d}, it appears that both $(h3,~r3)$ and $(h2,~r3)$ both have a higher avg.
runtime than $(h1,~r3)$.

Looking more at \mref{fig:res-100x20d}, it holds the same grouping by $r$ as we saw in \mref{fig:res-100x20k},
but the runtimes seems more stable from the beginning.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, page=1]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $k$, using all data from all test-runs with all matrices from this report.}
    \label{fig:res-all-data}
\end{figure}

Adding all the data from all matrices of all sizes together, we do se some of the significance of \mref{fig:res-100x20k}
bleeding trough, though an non-uniform test-case balance could be the reason for this. What's more important is the
overall runtime. Though the matrix size is not a huge factor, we do see that \mref{fig:res-1000x1000k} do impact
the result as some of it's worst cases was in the \textit{500Kms} or \textit{8+ minutes}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, page=8]{figures/Binary_matrix_runtime.pdf}
    \caption{Plot showing the average runtime of the algorithm in milliseconds \textit{(logarithmically)}
        over the parameter $d$, using all data from all test-runs with all matrices from this report, grouped by
        the size of the matrices.}
    \label{fig:res-all-matrices-over-d}
\end{figure}

Though the plot appears rather noisy in the larger $d$-space \textit{(due to the randomness of $d$)},
seeing all matrices by size over $d$, we can see the much of the same results as
in \mref{fig:res-all-matrices-over-k}, and it's clear that on average the $100 \times 20$ matrices perform
the worst.

% % % % % % % % % % % % % % % % %
%
%	6. Conclusion
%

\section{Conclusion}
\label{sec:conclusion}
By the performance seen during testing, it appears as if the algorithm behaves as expected. We saw only a polynomial
dependency on the input-matrix, and a much greater dependency on the given parameters $r$ and $k$.

However, it becomes apparent that this algorithm is in NP, as although inconceivably better than a brute-force
implementation, it still has a noticeable runtime, which can stretch from several minutes to hours even for small instances.
Thus the question of the algorithms viability still stands.

As for minor implementations of small matrices with little noise, and small parameters, it can be used on smaller devices,
and even smart-devices. However, any larger use-cases would wound up in minutes or hours, excluding the every-day use-cases.
Instead it could be of use in data-analysis, where larger machines are available, and where time is not limited in seconds.

As this implementation is only a "proof of concept", there are several theoretical improvements that should be tested to improve
runtime \textit{see \mref{sec:future-work}}. However, even with the current implementation, the results looks promising.

Still, more experimentation is needed.

\subsection{Future work}
\label{sec:future-work}
For stable results, the algorithm though theoretically able to be run in parallel, was only tested as a linear implementation,
due to hardware limitations on the testing computer producing highly irregular results for initial parallel tests.
Though the solutions produced were correct, the processing time varied too greatly between several runs, even on the same input.
Still with a different implementation it could theoretically be possible to optimize the algorithm further
for high parallel processing, maybe even GPU programming.

Most high-level programming languages have a performance drop when using recursion. While recursion is easy to implement, it can
usually be avoided with workarounds. If possible, the recursive calls in the main branching algorithm could be implemented
in an iterative procedure, possibly improving the runtime significantly.

The programming language was chosen for it's familiarity, and multi-OS practicality, as to produce a proof-of-concept.
It is well known, that the Java programming language is sub-optimal in its performance due to several
factors. The usage of a language which can get closer to the machine operations, would probably improve
runtime significantly.

The testing was done on "typical" home computers, which were both running the tests in the background, not
with the possibility of dedicated cores/threads. Though this has probably not affected the results unevenly,
it may have affected the overall runtime negatively. A dedicated processing server would eliminate the
possibility of operating systems, and random background tasks (updates etc) possibly
interfering with the results.

The hardware in the testing computers is of old technology. The average performance is probably no longer what
is promised by the manufacturer, and newer technology does on average have better performance. Also the writer
was made aware during testing that there existed a dispute on the processor technology in use for \mref{sec:computer};
namely the "AMD FX-8350" processor, where it was determined that it is in actuality not an 8-core, but 4-core.
\cite{dickey_v._advanced_micro_devices:inc._2015,dickey_v._advanced_micro_devices:inc._2020,chaim_gartenberg_2019}. This
has most likely impacted the accuracy of the runtime somewhat, when more than 4 test were running simultaneously on
the affected machine. However isolating the machine shows much of the same trends, but unfortunately holds to few test-cases to
be conclusive on it's own.

While the algorithm and it's implementation could theoretically be improved, the dataset used for testing was only
random. While working with proper datasets, natural symmetry and other patterns might make the data reducible enough,
or open for the possibility of customize the algorithm for a specific data-set.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % % % % % % % % % % % % % % % %
%
%	ADD REFERENCES
%
\newpage
\bibliography{extras/citation-db}
\bibliographystyle{apalike} %unsrt
\addcontentsline{toc}{section}{References}

\end{document}
